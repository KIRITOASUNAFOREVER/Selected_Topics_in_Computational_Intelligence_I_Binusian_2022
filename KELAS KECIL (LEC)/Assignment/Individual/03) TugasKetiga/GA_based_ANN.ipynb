{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GA_based_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R0YBXR61bwac"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import time\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \n",
        "        '''The list ``sizes`` contains the number of neurons in the\n",
        "        respective layers of the network.  For example, if the list\n",
        "        was [2, 3, 1] then it would be a three-layer network, with the\n",
        "        first layer containing 2 neurons, the second layer 3 neurons,\n",
        "        and the third layer 1 neuron.  The biases and weights for the\n",
        "        network are initialized randomly, using a Gaussian\n",
        "        distribution with mean 0, and variance 1.  Note that the first\n",
        "        layer is assumed to be an input layer, and by convention we\n",
        "        won't set any biases for those neurons, since biases are only\n",
        "        ever used in computing the outputs from later layers.'''\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        \n",
        "        # helper variables\n",
        "        self.bias_nitem = sum(sizes[1:])\n",
        "        self.weight_nitem = sum([self.weights[i].size for i in range(self.num_layers-2)])\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        '''Return the output of the network if ``a`` is input.'''\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = self.sigmoid(np.dot(w,a)+b)\n",
        "        return a\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''The sigmoid function.'''\n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    def score(self, X, y):\n",
        "\n",
        "        '''\n",
        "        @X = data to test\n",
        "        @y = data-label to test\n",
        "        @returns = score of network prediction (less is better)\n",
        "        @ref: https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
        "        '''\n",
        "\n",
        "        total_score=0\n",
        "        for i in range(X.shape[0]):\n",
        "            predicted = self.feedforward(X[i].reshape(-1,1))\n",
        "            actual = y[i].reshape(-1,1)\n",
        "            total_score += np.sum(np.power(predicted-actual,2)/2)  # mean-squared error\n",
        "        return total_score\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "\n",
        "        '''\n",
        "        @X = data to test\n",
        "        @y = data-label to test\n",
        "        @returns = accuracy (%) (more is better)\n",
        "        '''\n",
        "\n",
        "        accuracy = 0\n",
        "        for i in range(X.shape[0]):\n",
        "            output = self.feedforward(X[i].reshape(-1,1))\n",
        "            accuracy += int(np.argmax(output) == np.argmax(y[i]))\n",
        "        return accuracy / X.shape[0] * 100\n",
        "\n",
        "    def __str__(self):\n",
        "        s = \"\\nBias:\\n\\n\" + str(self.biases)\n",
        "        s += \"\\nWeights:\\n\\n\" + str(self.weights)\n",
        "        s += \"\\n\\n\"\n",
        "        return s\n",
        "\n",
        "class NNGeneticAlgo:\n",
        "\n",
        "    def __init__(self, n_pops, net_size, mutation_rate, crossover_rate, retain_rate, X, y):\n",
        "\n",
        "        '''\n",
        "        n_pops   = How much population do our GA need to create\n",
        "        net_size = Size of neural network for population members\n",
        "        mutation_rate = probability of mutating all bias & weight inside our network\n",
        "        crossover_rate = probability of cross-overing all bias & weight inside out network\n",
        "        retain_rate = How many to retain our population for the best ones\n",
        "        X = our data to test accuracy\n",
        "        y = our data-label to test accuracy\n",
        "        '''\n",
        "\n",
        "        self.n_pops = n_pops\n",
        "        self.net_size = net_size\n",
        "        self.nets = [Network(self.net_size) for i in range(self.n_pops)]\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.crossover_rate = crossover_rate\n",
        "        self.retain_rate = retain_rate\n",
        "        self.X = X[:]\n",
        "        self.y = y[:]\n",
        "    \n",
        "    def get_random_point(self, type):\n",
        "\n",
        "        '''\n",
        "        @type = either 'weight' or 'bias'\n",
        "        @returns tuple (layer_index, point_index)\n",
        "            note: if type is set to 'weight', point_index will return (row_index, col_index)\n",
        "        '''\n",
        "\n",
        "        nn = self.nets[0]\n",
        "        layer_index, point_index = random.randint(0, nn.num_layers-2), 0\n",
        "        if type == 'weight':\n",
        "            row = random.randint(0,nn.weights[layer_index].shape[0]-1)\n",
        "            col = random.randint(0,nn.weights[layer_index].shape[1]-1)\n",
        "            point_index = (row, col)\n",
        "        elif type == 'bias':\n",
        "            point_index = random.randint(0,nn.biases[layer_index].size-1)\n",
        "        return (layer_index, point_index)\n",
        "\n",
        "    def get_all_scores(self):\n",
        "        return [net.score(self.X, self.y) for net in self.nets]\n",
        "\n",
        "    def get_all_accuracy(self):\n",
        "        return [net.accuracy(self.X, self.y) for net in self.nets]\n",
        "\n",
        "    def crossover(self, father, mother):\n",
        "\n",
        "        '''\n",
        "        @father = neural-net object representing father\n",
        "        @mother = neural-net object representing mother\n",
        "        @returns = new child based on father/mother genetic information\n",
        "        '''\n",
        "\n",
        "        # make a copy of father 'genetic' weights & biases information\n",
        "        nn = copy.deepcopy(father)\n",
        "\n",
        "        # cross-over bias\n",
        "        for _ in range(self.nets[0].bias_nitem):\n",
        "            # get some random points\n",
        "            layer, point = self.get_random_point('bias')\n",
        "            # replace genetic (bias) with mother's value\n",
        "            if random.uniform(0,1) < self.crossover_rate:\n",
        "                nn.biases[layer][point] = mother.biases[layer][point]\n",
        "\n",
        "        # cross-over weight\n",
        "        for _ in range(self.nets[0].weight_nitem):\n",
        "            # get some random points\n",
        "            layer, point = self.get_random_point('weight')\n",
        "            # replace genetic (weight) with mother's value\n",
        "            if random.uniform(0,1) < self.crossover_rate:\n",
        "                nn.weights[layer][point] = mother.weights[layer][point]\n",
        "        \n",
        "        return nn\n",
        "        \n",
        "    def mutation(self, child):\n",
        "\n",
        "        '''\n",
        "        @child_index = neural-net object to mutate its internal weights & biases value\n",
        "        @returns = new mutated neural-net\n",
        "        '''\n",
        "\n",
        "        nn = copy.deepcopy(child)\n",
        "\n",
        "        # mutate bias\n",
        "        for _ in range(self.nets[0].bias_nitem):\n",
        "            # get some random points\n",
        "            layer, point = self.get_random_point('bias')\n",
        "            # add some random value between -0.5 and 0.5\n",
        "            if random.uniform(0,1) < self.mutation_rate:\n",
        "                nn.biases[layer][point] += random.uniform(-0.5, 0.5)\n",
        "\n",
        "        # mutate weight\n",
        "        for _ in range(self.nets[0].weight_nitem):\n",
        "            # get some random points\n",
        "            layer, point = self.get_random_point('weight')\n",
        "            # add some random value between -0.5 and 0.5\n",
        "            if random.uniform(0,1) < self.mutation_rate:\n",
        "                nn.weights[layer][point[0], point[1]] += random.uniform(-0.5, 0.5)\n",
        "\n",
        "        return nn\n",
        "\n",
        "    def evolve(self):\n",
        "\n",
        "        # calculate score for each population of neural-net\n",
        "        score_list = list(zip(self.nets, self.get_all_scores()))\n",
        "\n",
        "        # sort the network using its score\n",
        "        score_list.sort(key=lambda x: x[1])\n",
        "\n",
        "        # exclude score as it is not needed anymore\n",
        "        score_list = [obj[0] for obj in score_list]\n",
        "\n",
        "        # keep only the best one\n",
        "        retain_num = int(self.n_pops*self.retain_rate)\n",
        "        score_list_top = score_list[:retain_num]\n",
        "\n",
        "        # return some non-best ones\n",
        "        retain_non_best = int((self.n_pops-retain_num) * self.retain_rate)\n",
        "        for _ in range(random.randint(0, retain_non_best)):\n",
        "            score_list_top.append(random.choice(score_list[retain_num:]))\n",
        "\n",
        "        # breed new childs if current population number less than what we want\n",
        "        while len(score_list_top) < self.n_pops:\n",
        "\n",
        "            father = random.choice(score_list_top)\n",
        "            mother = random.choice(score_list_top)\n",
        "\n",
        "            if father != mother:\n",
        "                new_child = self.crossover(father, mother)\n",
        "                new_child = self.mutation(new_child)\n",
        "                score_list_top.append(new_child)\n",
        "        \n",
        "        # copy our new population to current object\n",
        "        self.nets = score_list_top"
      ],
      "metadata": {
        "id": "ATcsL_w3cA3U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = []\n",
        "def main():\n",
        "\n",
        "    # load data from iris.csv into X and y\n",
        "    df = pd.read_csv(\"penguins.csv\")\n",
        "    X = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values.astype(int)\n",
        "    y = df['species'].values\n",
        "\n",
        "    print(X)\n",
        "    print(y)\n",
        "    # convert y into one-hot encoded format\n",
        "    y = y.reshape(-1, 1)\n",
        "    enc = OneHotEncoder()\n",
        "    enc.fit(y)\n",
        "    y = enc.transform(y).toarray()\n",
        "    print(y)\n",
        "    # parameters\n",
        "    N_POPS = 30\n",
        "    NET_SIZE = [4,20,3] \n",
        "    MUTATION_RATE = 0.2\n",
        "    CROSSOVER_RATE = 0.4\n",
        "    RETAIN_RATE = 0.4\n",
        "\n",
        "    # start our neural-net & optimize it using genetic algorithm\n",
        "    nnga = NNGeneticAlgo(N_POPS, NET_SIZE, MUTATION_RATE, CROSSOVER_RATE, RETAIN_RATE, X, y)\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # run for n iterations\n",
        "    for i in range(1000):\n",
        "        accuracy.append(nnga.get_all_accuracy()[0])\n",
        "        if i % 10 == 0:\n",
        "            print(\"Current iteration : {}\".format(i+1))\n",
        "            print(\"Time taken by far : %.1f seconds\" % (time.time() - start_time))\n",
        "            print(\"Current top member's network accuracy: %.2f%%\\n\" % nnga.get_all_accuracy()[0])\n",
        "\n",
        "        # evolve the population\n",
        "        nnga.evolve()"
      ],
      "metadata": {
        "id": "7U74PIV4cG9P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtKOBR3vcIHi",
        "outputId": "254b5fa0-b930-4f9a-86f8-b67260d0f263"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  39   18  181 3750]\n",
            " [  39   17  186 3800]\n",
            " [  40   18  195 3250]\n",
            " ...\n",
            " [  49   18  193 3775]\n",
            " [  50   19  210 4100]\n",
            " [  50   18  198 3775]]\n",
            "['Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie'\n",
            " 'Adelie' 'Adelie' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo' 'Gentoo'\n",
            " 'Gentoo' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap' 'Chinstrap'\n",
            " 'Chinstrap' 'Chinstrap' 'Chinstrap']\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current iteration : 1\n",
            "Time taken by far : 0.2 seconds\n",
            "Current top member's network accuracy: 43.84%\n",
            "\n",
            "Current iteration : 11\n",
            "Time taken by far : 6.1 seconds\n",
            "Current top member's network accuracy: 35.74%\n",
            "\n",
            "Current iteration : 21\n",
            "Time taken by far : 12.1 seconds\n",
            "Current top member's network accuracy: 43.84%\n",
            "\n",
            "Current iteration : 31\n",
            "Time taken by far : 18.1 seconds\n",
            "Current top member's network accuracy: 43.84%\n",
            "\n",
            "Current iteration : 41\n",
            "Time taken by far : 24.8 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 51\n",
            "Time taken by far : 30.5 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 61\n",
            "Time taken by far : 40.9 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 71\n",
            "Time taken by far : 46.8 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 81\n",
            "Time taken by far : 52.7 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 91\n",
            "Time taken by far : 58.7 seconds\n",
            "Current top member's network accuracy: 48.35%\n",
            "\n",
            "Current iteration : 101\n",
            "Time taken by far : 64.5 seconds\n",
            "Current top member's network accuracy: 67.87%\n",
            "\n",
            "Current iteration : 111\n",
            "Time taken by far : 70.5 seconds\n",
            "Current top member's network accuracy: 67.87%\n",
            "\n",
            "Current iteration : 121\n",
            "Time taken by far : 76.4 seconds\n",
            "Current top member's network accuracy: 53.15%\n",
            "\n",
            "Current iteration : 131\n",
            "Time taken by far : 82.3 seconds\n",
            "Current top member's network accuracy: 66.37%\n",
            "\n",
            "Current iteration : 141\n",
            "Time taken by far : 88.0 seconds\n",
            "Current top member's network accuracy: 74.17%\n",
            "\n",
            "Current iteration : 151\n",
            "Time taken by far : 93.8 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 161\n",
            "Time taken by far : 100.7 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 171\n",
            "Time taken by far : 106.5 seconds\n",
            "Current top member's network accuracy: 77.78%\n",
            "\n",
            "Current iteration : 181\n",
            "Time taken by far : 112.3 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 191\n",
            "Time taken by far : 118.3 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 201\n",
            "Time taken by far : 124.1 seconds\n",
            "Current top member's network accuracy: 77.48%\n",
            "\n",
            "Current iteration : 211\n",
            "Time taken by far : 131.4 seconds\n",
            "Current top member's network accuracy: 78.68%\n",
            "\n",
            "Current iteration : 221\n",
            "Time taken by far : 137.4 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 231\n",
            "Time taken by far : 143.3 seconds\n",
            "Current top member's network accuracy: 76.58%\n",
            "\n",
            "Current iteration : 241\n",
            "Time taken by far : 149.1 seconds\n",
            "Current top member's network accuracy: 76.58%\n",
            "\n",
            "Current iteration : 251\n",
            "Time taken by far : 154.7 seconds\n",
            "Current top member's network accuracy: 76.58%\n",
            "\n",
            "Current iteration : 261\n",
            "Time taken by far : 160.7 seconds\n",
            "Current top member's network accuracy: 78.68%\n",
            "\n",
            "Current iteration : 271\n",
            "Time taken by far : 166.5 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 281\n",
            "Time taken by far : 172.2 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 291\n",
            "Time taken by far : 178.1 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 301\n",
            "Time taken by far : 184.0 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 311\n",
            "Time taken by far : 189.8 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 321\n",
            "Time taken by far : 195.6 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 331\n",
            "Time taken by far : 201.2 seconds\n",
            "Current top member's network accuracy: 75.08%\n",
            "\n",
            "Current iteration : 341\n",
            "Time taken by far : 206.9 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 351\n",
            "Time taken by far : 212.7 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 361\n",
            "Time taken by far : 218.7 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 371\n",
            "Time taken by far : 224.6 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 381\n",
            "Time taken by far : 230.3 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 391\n",
            "Time taken by far : 236.1 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 401\n",
            "Time taken by far : 241.9 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 411\n",
            "Time taken by far : 247.7 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 421\n",
            "Time taken by far : 253.6 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 431\n",
            "Time taken by far : 259.3 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 441\n",
            "Time taken by far : 265.1 seconds\n",
            "Current top member's network accuracy: 78.08%\n",
            "\n",
            "Current iteration : 451\n",
            "Time taken by far : 270.8 seconds\n",
            "Current top member's network accuracy: 77.48%\n",
            "\n",
            "Current iteration : 461\n",
            "Time taken by far : 276.6 seconds\n",
            "Current top member's network accuracy: 77.48%\n",
            "\n",
            "Current iteration : 471\n",
            "Time taken by far : 282.3 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 481\n",
            "Time taken by far : 288.1 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 491\n",
            "Time taken by far : 293.8 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 501\n",
            "Time taken by far : 299.5 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 511\n",
            "Time taken by far : 305.3 seconds\n",
            "Current top member's network accuracy: 77.78%\n",
            "\n",
            "Current iteration : 521\n",
            "Time taken by far : 311.1 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 531\n",
            "Time taken by far : 316.9 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 541\n",
            "Time taken by far : 322.6 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 551\n",
            "Time taken by far : 328.4 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 561\n",
            "Time taken by far : 334.0 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 571\n",
            "Time taken by far : 339.9 seconds\n",
            "Current top member's network accuracy: 77.78%\n",
            "\n",
            "Current iteration : 581\n",
            "Time taken by far : 345.8 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 591\n",
            "Time taken by far : 351.6 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 601\n",
            "Time taken by far : 357.3 seconds\n",
            "Current top member's network accuracy: 78.38%\n",
            "\n",
            "Current iteration : 611\n",
            "Time taken by far : 363.1 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 621\n",
            "Time taken by far : 368.8 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 631\n",
            "Time taken by far : 374.6 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 641\n",
            "Time taken by far : 380.2 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 651\n",
            "Time taken by far : 386.0 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 661\n",
            "Time taken by far : 391.8 seconds\n",
            "Current top member's network accuracy: 78.98%\n",
            "\n",
            "Current iteration : 671\n",
            "Time taken by far : 397.6 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 681\n",
            "Time taken by far : 403.5 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 691\n",
            "Time taken by far : 409.3 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 701\n",
            "Time taken by far : 415.0 seconds\n",
            "Current top member's network accuracy: 79.58%\n",
            "\n",
            "Current iteration : 711\n",
            "Time taken by far : 420.7 seconds\n",
            "Current top member's network accuracy: 79.28%\n",
            "\n",
            "Current iteration : 721\n",
            "Time taken by far : 426.4 seconds\n",
            "Current top member's network accuracy: 76.88%\n",
            "\n",
            "Current iteration : 731\n",
            "Time taken by far : 432.2 seconds\n",
            "Current top member's network accuracy: 89.49%\n",
            "\n",
            "Current iteration : 741\n",
            "Time taken by far : 438.1 seconds\n",
            "Current top member's network accuracy: 89.79%\n",
            "\n",
            "Current iteration : 751\n",
            "Time taken by far : 444.0 seconds\n",
            "Current top member's network accuracy: 90.09%\n",
            "\n",
            "Current iteration : 761\n",
            "Time taken by far : 449.8 seconds\n",
            "Current top member's network accuracy: 93.69%\n",
            "\n",
            "Current iteration : 771\n",
            "Time taken by far : 455.6 seconds\n",
            "Current top member's network accuracy: 90.99%\n",
            "\n",
            "Current iteration : 781\n",
            "Time taken by far : 461.4 seconds\n",
            "Current top member's network accuracy: 90.99%\n",
            "\n",
            "Current iteration : 791\n",
            "Time taken by far : 469.5 seconds\n",
            "Current top member's network accuracy: 90.99%\n",
            "\n",
            "Current iteration : 801\n",
            "Time taken by far : 476.8 seconds\n",
            "Current top member's network accuracy: 90.99%\n",
            "\n",
            "Current iteration : 811\n",
            "Time taken by far : 482.5 seconds\n",
            "Current top member's network accuracy: 90.99%\n",
            "\n",
            "Current iteration : 821\n",
            "Time taken by far : 488.2 seconds\n",
            "Current top member's network accuracy: 91.89%\n",
            "\n",
            "Current iteration : 831\n",
            "Time taken by far : 494.1 seconds\n",
            "Current top member's network accuracy: 91.29%\n",
            "\n",
            "Current iteration : 841\n",
            "Time taken by far : 499.8 seconds\n",
            "Current top member's network accuracy: 90.09%\n",
            "\n",
            "Current iteration : 851\n",
            "Time taken by far : 505.4 seconds\n",
            "Current top member's network accuracy: 90.09%\n",
            "\n",
            "Current iteration : 861\n",
            "Time taken by far : 511.1 seconds\n",
            "Current top member's network accuracy: 90.09%\n",
            "\n",
            "Current iteration : 871\n",
            "Time taken by far : 516.8 seconds\n",
            "Current top member's network accuracy: 91.89%\n",
            "\n",
            "Current iteration : 881\n",
            "Time taken by far : 522.5 seconds\n",
            "Current top member's network accuracy: 91.89%\n",
            "\n",
            "Current iteration : 891\n",
            "Time taken by far : 528.3 seconds\n",
            "Current top member's network accuracy: 92.49%\n",
            "\n",
            "Current iteration : 901\n",
            "Time taken by far : 534.0 seconds\n",
            "Current top member's network accuracy: 92.49%\n",
            "\n",
            "Current iteration : 911\n",
            "Time taken by far : 539.9 seconds\n",
            "Current top member's network accuracy: 92.49%\n",
            "\n",
            "Current iteration : 921\n",
            "Time taken by far : 546.8 seconds\n",
            "Current top member's network accuracy: 89.79%\n",
            "\n",
            "Current iteration : 931\n",
            "Time taken by far : 552.6 seconds\n",
            "Current top member's network accuracy: 92.19%\n",
            "\n",
            "Current iteration : 941\n",
            "Time taken by far : 558.5 seconds\n",
            "Current top member's network accuracy: 92.19%\n",
            "\n",
            "Current iteration : 951\n",
            "Time taken by far : 564.3 seconds\n",
            "Current top member's network accuracy: 92.79%\n",
            "\n",
            "Current iteration : 961\n",
            "Time taken by far : 570.2 seconds\n",
            "Current top member's network accuracy: 92.79%\n",
            "\n",
            "Current iteration : 971\n",
            "Time taken by far : 576.1 seconds\n",
            "Current top member's network accuracy: 92.79%\n",
            "\n",
            "Current iteration : 981\n",
            "Time taken by far : 581.9 seconds\n",
            "Current top member's network accuracy: 93.39%\n",
            "\n",
            "Current iteration : 991\n",
            "Time taken by far : 587.8 seconds\n",
            "Current top member's network accuracy: 93.39%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(accuracy)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AJe3i4iMC8eZ",
        "outputId": "a3671ce8-0d6c-4a1f-af3f-455e58c33c7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}